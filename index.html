<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="ELMUR">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="ELMUR is a transformer model with layer-local external memory and LRU-based memory updates for long-horizon reasoning in POMDPs">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="ELMUR, external memory, long-horizon RL, transformer, reinforcement learning, machine learning, AI, robotics, POMDPs">
  <!-- TODO: List all authors -->
  <meta name="author" content="Egor Cherepanov, Alexey Kovalev, Aleksandr Panov">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Cognitive AI Lab">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="ELMUR">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="ELMUR is a transformer model with layer-local external memory and LRU-based memory updates for long-horizon reasoning in POMDPs">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://elmur-paper.github.io/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="ELMUR - Research Preview">
  <meta property="article:published_time" content="2025-01-01T00:00:00.000Z">
  <meta property="article:author" content="Egor Cherepanov">
  <meta property="article:section" content="Machine Learning">
  <meta property="article:tag" content="Reinforcement Learning">
  <meta property="article:tag" content="Long-Horizon RL">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="ELMUR is a transformer model with layer-local external memory and LRU-based memory updates for long-horizon reasoning in POMDPs">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="ELMUR - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL">
  <meta name="citation_author" content="Cherepanov, Egor">
  <meta name="citation_author" content="Kovalev, Alexey K.">
  <meta name="citation_author" content="Panov, Aleksandr I.">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_journal_title" content="arXiv preprint arXiv:2510.07151">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2510.07151.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>ELMUR</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link rel="apple-touch-icon" href="static/images/icon.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL",
    "description": "ELMUR is a transformer model with layer-local external memory and LRU-based memory updates for long-horizon reasoning in POMDPs",
    "author": [
      {
        "@type": "Person",
        "name": "Egor Cherepanov",
        "affiliation": {
          "@type": "Organization",
          "name": "Cognitive AI Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Alexey K. Kovalev",
        "affiliation": {
          "@type": "Organization",
          "name": "Cognitive AI Lab"
        }
      },
      {
        "@type": "Person",
        "name": "Aleksandr I. Panov",
        "affiliation": {
          "@type": "Organization",
          "name": "Cognitive AI Lab"
        }
      }
    ],
    "datePublished": "2025",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://arxiv.org/abs/2510.07151",
    "sameAs": "https://arxiv.org/abs/2510.07151",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["machine learning", "reinforcement learning", "long-horizon RL", "transformer", "external memory"],
    "abstract": "Real-world robotic agents must act under partial observability and long horizons, where key cues may appear long before they affect decision making. However, most modern approaches rely solely on instantaneous information, without incorporating insights from the past. Standard recurrent or transformer models struggle with retaining and leveraging long-term dependencies: context windows truncate history, while naive memory extensions fail under scale and sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a transformer architecture with structured external memory. Each layer maintains memory embeddings, interacts with them via bidirectional cross-attention, and updates them through an Least Recently Used (LRU) memory module using replacement or convex blending. ELMUR extends effective horizons up to 100,000 times beyond the attention window and achieves a 100% success rate on a synthetic T-Maze task with corridors up to one million steps. In POPGym, it outperforms baselines on more than half of the tasks. On MIKASA-Robo sparse-reward manipulation tasks with visual observations, it nearly doubles the performance of strong baselines. These results demonstrate that structured, layer-local external memory offers a simple and scalable approach to decision making under partial observability.",
    "citation": "@misc{cherepanov2025elmurexternallayermemory, title={ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL}, author={Egor Cherepanov and Alexey K. Kovalev and Aleksandr I. Panov}, year={2025}, eprint={2510.07151}, archivePrefix={arXiv}, primaryClass={cs.LG}, url={https://arxiv.org/abs/2510.07151}}",
    "isAccessibleForFree": true,
    "license": "https://arxiv.org/licenses/assumed",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://elmur-paper.github.io/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Machine Learning"
      },
      {
        "@type": "Thing",
        "name": "Reinforcement Learning"
      },
      {
        "@type": "Thing",
        "name": "Long-Horizon Reasoning"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>

  <!-- Animated Parallax Background -->
  <div class="parallax-background">
    <div class="parallax-layer layer-1">
      <div class="floating-shape circle large" style="top: 10%; left: 5%;"></div>
      <div class="floating-shape triangle medium" style="top: 20%; right: 8%;"></div>
      <div class="floating-shape square small" style="top: 70%; left: 15%;"></div>
      <div class="floating-shape circle medium" style="top: 50%; right: 20%;"></div>
    </div>
    <div class="parallax-layer layer-2">
      <div class="floating-shape square large" style="top: 30%; left: 25%;"></div>
      <div class="floating-shape circle small" style="top: 80%; right: 5%;"></div>
      <div class="floating-shape triangle large" style="top: 15%; left: 70%;"></div>
      <div class="floating-shape square medium" style="top: 60%; right: 30%;"></div>
    </div>
    <div class="parallax-layer layer-3">
      <div class="floating-shape circle small" style="top: 40%; left: 50%;"></div>
      <div class="floating-shape triangle small" style="top: 25%; left: 10%;"></div>
      <div class="floating-shape square small" style="top: 85%; right: 15%;"></div>
      <div class="floating-shape circle large" style="top: 75%; right: 40%;"></div>
    </div>
    <div class="parallax-layer layer-4">
      <div class="floating-shape triangle medium" style="top: 5%; left: 40%;"></div>
      <div class="floating-shape square large" style="top: 45%; right: 50%;"></div>
      <div class="floating-shape circle medium" style="top: 90%; left: 80%;"></div>
    </div>
    <div class="parallax-layer layer-5">
      <div class="floating-shape square medium" style="top: 35%; left: 60%;"></div>
      <div class="floating-shape circle small" style="top: 65%; right: 60%;"></div>
      <div class="floating-shape triangle small" style="top: 55%; left: 30%;"></div>
    </div>
  </div>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More about memory in RL/Robotics
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: Replace with your lab's related works -->
        <a href="https://arxiv.org/pdf/2502.10550" class="work-item" target="_blank">
          <div class="work-info">
            <!-- TODO: Replace with actual paper title -->
            <h5>Memory, Benchmark & Robots: a Benchmark for Solving Complex Tasks with Reinforcement Learning</h5>
            <!-- TODO: Replace with brief description -->
            <p>MIKASA-Robo: a benchmark with memory-intensive tabletop robotic manipulation tasks.</p>
            <!-- TODO: Replace with venue and year -->
            <!-- <span class="work-venue">Conference/Journal 2024</span> -->
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <!-- TODO: Add more related works or remove extra items -->
        <a href="https://arxiv.org/pdf/2306.09459" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Recurrent Action Transformer with Memory</h5>
            <p>RATE: a memory-augmented transformer for long-horizon Offline RL.</p>
            <!-- <span class="work-venue">Conference/Journal 2023</span> -->
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/pdf/2412.06531?" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Unraveling the Complexity of Memory in RL Agents: an Approach for Classification and Evaluation</h5>
            <p>A theoretical framework for separating different types of memory in RL.</p>
            <!-- <span class="work-venue">Conference/Journal 2023</span> -->
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://avanturist322.github.io/" target="_blank">Egor Cherepanov</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=N1zWb74AAAAJ" target="_blank">Alexey K. Kovalev</a><sup>1,2</sup>,</span>
                  <span class="author-block">
                    <a href="https://grafft.github.io/" target="_blank">Aleksandr I. Panov</a><sup>1,2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <sup>1</sup>Cognitive AI Lab, Moscow, Russia, <sup>2</sup>IAI MIPT, Moscow, Russia<br>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="award-banner" style="color: #28a745; font-weight: 600; margin: 10px 0;">
                      🏆 <a href="https://rememberl-corl25.github.io/" target="_blank" style="color: #28a745; text-decoration: underline;">RemembeRL Workshop</a> @ CoRL 2025, Best Poster Award & Spotlight
                    </div>
                    <div class="publication-links">
                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2510.07151" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>

                  <a href="https://arxiv.org/pdf/2510.07151.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/CognitiveAISystems/RATE/tree/main/offline_rl_baselines/ELMUR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>


              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Like Button - Bottom Left Corner -->
<div class="like-container-bottom-left" id="likeContainer" style="position: fixed; bottom: 2rem; left: 2rem; z-index: 1000; background: rgba(255, 255, 255, 0.95); padding: 1rem 1.5rem; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.15); backdrop-filter: blur(10px); -webkit-backdrop-filter: blur(10px);">
  <button id="likeBtn" onclick="toggleLike()" style="background: none; border: none; cursor: pointer; font-size: 1rem; transition: all 0.3s ease; display: flex; align-items: center; gap: 0.5rem;">
    <i id="likeIcon" class="far fa-heart" style="color: #e53e3e; font-size: 1.5rem;"></i>
    <div>
      <div id="likeText" style="font-weight: 500; white-space: nowrap;">Like this work!</div>
      <!-- Like stats removed from UI -->
    </div>
  </button>
</div>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop" style="max-width: 1100px;">
    <div class="hero-body">
      <!-- Main model image -->
      <img src="static/images/main-model.png" alt="ELMUR Main Model Architecture" style="width: 100%; height: auto; border-radius: var(--border-radius-lg); box-shadow: var(--shadow-xl); margin: 1rem 0;" />
      <!-- TODO: Replace with your image description -->
      <h2 class="subtitle has-text-centered">
        <strong>ELMUR overview.</strong> Each transformer layer is augmented with an external memory track that runs in parallel with the token track. Tokens attend to memory through a <code>mem2tok</code> block, while memory embeddings are updated from tokens through a <code>tok2mem</code> block. LRU block selectively rewrites memory via replacement or convex blending, ensuring bounded yet persistent storage. This design enables token-memory interaction and long-horizon recall beyond the attention window.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Real-world robotic agents must act under partial observability and long horizons, where key cues may appear long before they affect decision making. However, most modern approaches rely solely on instantaneous information, without incorporating insights from the past. Standard recurrent or transformer models struggle with retaining and leveraging long-term dependencies: context windows truncate history, while naive memory extensions fail under scale and sparsity.
          </p>
          <p>
            We propose <strong>ELMUR</strong> (<strong>E</strong>xternal <strong>L</strong>ayer <strong>M</strong>emory with <strong>U</strong>pdate/<strong>R</strong>ewrite), a transformer architecture with structured external memory. Each layer maintains memory embeddings, interacts with them via bidirectional cross-attention, and updates them through an <strong>L</strong>east <strong>R</strong>ecently <strong>U</strong>sed <strong>(LRU)</strong> memory module using replacement or convex blending.
          </p>
          <p>
            ELMUR extends effective horizons up to 100,000 times beyond the attention window and achieves a 100% success rate on a synthetic T-Maze task with corridors up to one million steps. In POPGym, it outperforms baselines on more than half of the tasks. On MIKASA-Robo sparse-reward manipulation tasks with visual observations, it nearly doubles the performance of strong baselines. These results demonstrate that structured, layer-local external memory offers a simple and scalable approach to decision making under partial observability.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- ===================== METHOD (condensed) ===================== -->
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<section class="section" id="method">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Method</h2>

    <div style="float: right; width: 50%; margin-left: 1rem; margin-bottom: 1rem;">
      <img src="static/images/lru-scheme.png" alt="LRU-based memory management" style="width:100%; margin-top:0.5rem;">
      <p class="has-text-centered is-size-7">
        <em>LRU: fill empty slots, then blend updates into the least-recently-used slot.</em>
      </p>
    </div>

    <div class="content">
      <p>
        Long-horizon, partially observed tasks need persistent memory beyond a fixed attention window.
        <strong>ELMUR</strong> augments each Transformer layer with <em>layer-local external memory</em> with cross-attention-based read–write operations managed by an Least Recently Used (LRU) policy which selectively updates memory via replacement or convex blending.
      </p>

      <h3 class="title is-5">Segment-Level Recurrence</h3>
      <p>
        A trajectory of length \(T\) is split into \(S=\lceil T/L\rceil\) segments \(\mathcal{S}_i\) (context \(L\)).
        Each segment reads the previous memory (detached) and updates it, where \(\mathbf{h}^{(i)}\) represents the token embeddings at segment \(i\) and \(\mathbf{m}^{i-1}\) represents the memory embeddings from the previous segment:
      </p>
      <p>
        \[
          \mathbf{h}^{(i)}=\mathrm{TokenTrack}\!\big(\mathcal{S}_i,\operatorname{sg}(\mathbf{m}^{\,i-1})\big).
        \]
      </p>

      <h3 class="title is-5">Token Track (Read)</h3>
      <p>
        Tokens model local structure with self-attention and read from memory via cross-attention:
      </p>
      <p>
        \[
          \mathbf{h}_{\text{sa}}=\mathrm{AddNorm}\!\big(\mathbf{x}+\mathrm{SelfAttention}(\mathbf{x})\big),
        \]
        \[
          \mathbf{h}_{\text{mem2tok}}=\mathrm{AddNorm}\!\big(\mathbf{h}_{\text{sa}}+\mathrm{CrossAttention}(Q{=}\mathbf{h}_{\text{sa}},K,V{=}\mathbf{m})\big),
        \]
        \[
          \mathbf{h}=\mathrm{AddNorm}\!\big(\mathbf{h}_{\text{mem2tok}}+\mathrm{FFN}(\mathbf{h}_{\text{mem2tok}})\big).
        \]
      </p>

      <h3 class="title is-5">Memory Track (Write)</h3>
      <p>
        Memory is updated from tokens via cross-attention and FFN:
      </p>
      <p>
        \[
          \mathbf{m}_{\text{tok2mem}}=\mathrm{AddNorm}\!\big(\mathbf{m}+\mathrm{CrossAttention}(Q{=}\mathbf{m},K,V{=}\mathbf{h})\big),
        \]
        \[
          \mathbf{m}_{\text{new}}=\mathrm{AddNorm}\!\big(\mathbf{m}_{\text{tok2mem}}+\mathrm{FFN}(\mathbf{m}_{\text{tok2mem}})\big).
        \]
      </p>

      <h3 class="title is-5">Relative Bias (Read/Write)</h3>
      <p>
        Cross-attention uses a learned relative bias to encode token–memory offsets:
      </p>
      <p>
        \[
          \mathrm{Attn}(\mathbf{Q},\mathbf{K})=\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_h}}+\mathbf{B}_{\text{rel}},
        \qquad
          \mathbf{B}_{\text{rel}}=
          \begin{cases}
            \mathbf{E}[t{-}p]\in\mathbb{R}^{B\times H\times L\times M} & \text{read (mem2tok)}\\[4pt]
            \mathbf{E}[p{-}t]\in\mathbb{R}^{B\times H\times M\times L} & \text{write (tok2mem)}
          \end{cases}
        \]
      </p>

      <h3 class="title is-5">LRU Memory Update (Bounded & Persistent)</h3>
      <p>
        Empty slots are filled; otherwise the least-recently-used slot is refreshed by convex blending:
      </p>
      <p>
        \[
          \mathbf{m}^{\,i+1}_j=\lambda\,\mathbf{m}^{\,i+1}_{\text{new}}+(1-\lambda)\,\mathbf{m}^{\,i}_j,\quad \lambda\in[0,1].
        \]
      </p>
      <p>
        This yields scalable, temporally grounded memory with constant compute per segment.
      </p>


    </div>
  </div>
</section>





  <!-- ===================== RESULTS ===================== -->
  <section class="section" id="results">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experimental Results</h2>

    <div class="content has-text-justified">
      <p>
        We evaluate <strong>ELMUR</strong> on three memory-intensive benchmarks with partial observability:
        <strong><a href="https://arxiv.org/pdf/2307.03864" target="_blank" style="color: #2563eb;">T-Maze</a></strong> (synthetic long-horizon recall), <strong><a href="https://arxiv.org/pdf/2303.01859" target="_blank" style="color: #2563eb;">POPGym-48</a></strong> (33 memory puzzles + 15 reactive control tasks),
        and <strong><a href="https://arxiv.org/pdf/2502.10550" target="_blank" style="color: #2563eb;">MIKASA-Robo</a></strong> (sparse-reward robotic manipulation with RGB observations).
        Unless noted, results aggregate 3 runs (4 for T-Maze), 100 eval seeds per run, reported as mean&nbsp;±&nbsp;SEM.
      </p>
    </div>

    <!-- T-Maze -->
    <h3 class="title is-4">T-Maze: Long-Horizon Retention</h3>
    <div class="image-container" style="position: relative; overflow: hidden; border-radius: 12px;">
      <img src="static/images/tmaze-limits-elmur.png" alt="T-Maze long-horizon results" style="width:100%; transition: all 0.3s ease;" class="interactive-image">
      <div class="image-overlay" style="position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: linear-gradient(135deg, rgba(37, 99, 235, 0.1) 0%, rgba(124, 58, 237, 0.1) 100%); opacity: 0; transition: opacity 0.3s ease; display: flex; align-items: center; justify-content: center; color: white; font-size: 1.2rem; font-weight: 600;">
        🏆 100% Success Rate up to 1M steps!
      </div>
    </div>
    <p class="has-text-centered" style="margin-bottom: 2rem;">
      <em><strong>Success rate on the T-Maze task as a function of inference corridor length. </strong>
        ELMUR achieves a <strong>100% success rate</strong> up to corridor lengths of <strong>one million steps</strong>. In this figure, the context length is L=10 with S=3 segments; thus <strong>ELMUR carries information across horizons 100,000 times longer than its context window</strong>.</em>
    </p>
    <div class="content has-text-justified">
      <p>
        Despite being trained with a short attention window (L=10), ELMUR maintains perfect success as the corridor length increases from \(10^2\) to \(10^6\) steps, while standard sequence models rapidly degrade once the horizon exceeds their window. This shows that the external layer memory, not the attention cache, carries the cue over extreme delays.
      </p>
    </div>

    <div class="image-container" style="position: relative; overflow: hidden; border-radius: 12px;">
      <img src="static/images/heatmap.png" alt="Heatmap of T-Maze results" style="width:100%; transition: all 0.3s ease;" class="interactive-image">
      <div class="image-overlay" style="position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: linear-gradient(135deg, rgba(16, 185, 129, 0.1) 0%, rgba(5, 150, 105, 0.1) 100%); opacity: 0; transition: opacity 0.3s ease; display: flex; align-items: center; justify-content: center; color: white; font-size: 1.2rem; font-weight: 600;">
        📊 Perfect generalization across lengths!
      </div>
    </div>
    <p class="has-text-centered" style="margin-bottom: 2rem;">
      <em><strong>Generalization of ELMUR across T-Maze lengths.</strong>
        Each cell shows success rate (mean ± standard error) for training vs. validation lengths.
        ELMUR transfers perfectly: models trained on shorter sequences retain 100% success up to 9600 steps. Training lengths were split into three equal segments.</em>
    </p>
    <div class="content has-text-justified">
      <p>
        Models trained on short sequences (9–900 steps) extrapolate to much longer ones (up to 9600) without loss, and also interpolate to shorter lengths. This indicates that the learned read–write policy and relative bias remain stable across scales.
      </p>
    </div>

    <!-- POPGym -->
    <h3 class="title is-4 mt-6">POPGym-48: Broad Generalization</h3>

    <div style="text-align: center;">
      <div class="image-container" style="position: relative; overflow: hidden; border-radius: 12px; display: inline-block;">
      <img src="static/images/popgym-table.png" alt="POPGym table" style="width:70%; transition: all 0.3s ease;" class="interactive-image">
      <div class="image-overlay" style="position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: linear-gradient(135deg, rgba(245, 101, 101, 0.1) 0%, rgba(220, 38, 38, 0.1) 100%); opacity: 0; transition: opacity 0.3s ease; display: flex; align-items: center; justify-content: center; color: white; font-size: 1.2rem; font-weight: 600;">
        🏆 #1 on 24/48 POPGym tasks!
      </div>
    </div>
      <p class="has-text-centered" style="margin-bottom: 2rem;">
        <em><strong>Aggregated returns over all 48 POPGym tasks.</strong> ELMUR ranks <strong>first on 24/48 tasks</strong>, with the largest gains on memory puzzles.</em>
      </p>
    </div>
    <div class="content has-text-justified">
      <p>
        ELMUR achieves the best overall score on POPGym
        (<strong>10.4</strong>), driven by clear gains on the <em>puzzle</em> subset, while remaining competitive on <em>reactive</em> control.
        The improvements concentrate on tasks requiring long-term recall, validating the role of layer-local memory.
      </p>
    </div>

    <div class="image-container" style="position: relative; overflow: hidden; border-radius: 12px;">
      <img src="static/images/elmur-vs-dt.png" alt="POPGym results" style="width:100%; transition: all 0.3s ease;" class="interactive-image">
      <div class="image-overlay" style="position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: linear-gradient(135deg, rgba(139, 69, 19, 0.1) 0%, rgba(160, 82, 45, 0.1) 100%); opacity: 0; transition: opacity 0.3s ease; display: flex; align-items: center; justify-content: center; color: white; font-size: 1.2rem; font-weight: 600;">
        📈 Consistent improvements over DT!
      </div>
    </div>
    <p class="has-text-centered" style="margin-bottom: 2rem;">
      <em><strong>ELMUR compared to DT on all 48 POPGym tasks.</strong> Each model was trained with three independent runs, validated over 100 episodes each. Bars show the mean performance with 95% confidence intervals computed over these three means. ELMUR achieves consistent improvements over DT, with the largest gains on memory-intensive puzzles.</em>
    </p>

    <!-- MIKASA-Robo -->
    <h3 class="title is-4 mt-6">MIKASA-Robo: Visual Manipulation</h3>
    <div class="image-container" style="position: relative; overflow: hidden; border-radius: 12px;">
      <img src="static/images/mikasa-table.png" alt="MIKASA-Robo results" style="width:100%; transition: all 0.3s ease;" class="interactive-image">
      <div class="image-overlay" style="position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: linear-gradient(135deg, rgba(147, 51, 234, 0.1) 0%, rgba(126, 34, 206, 0.1) 100%); opacity: 0; transition: opacity 0.3s ease; display: flex; align-items: center; justify-content: center; color: white; font-size: 1.2rem; font-weight: 600;">
        🤖 Nearly 2x better than baselines!
      </div>
    </div>
    <p class="has-text-centered" style="margin-bottom: 2rem;">
      <em><strong>Success rates on MIKASA-Robo tasks.</strong> ELMUR nearly doubles the performance of strong baselines.</em>
    </p>
    <div class="content has-text-justified">
      <p>
        On sparse-reward manipulation, ELMUR attains
        <code>RememberColor3/5/9-v0</code> = <strong>0.89 ± 0.07</strong>, <strong>0.19 ± 0.03</strong>, <strong>0.23 ± 0.02</strong>, and
        <code>TakeItBack-v0</code> = <strong>0.78 ± 0.03</strong>, substantially above the best baselines (e.g., RATE 0.42 ± 0.24 on <code>TakeItBack</code>).
        The gains persist as distractors increase, suggesting that persistent per-layer memory resists visual interference better than windowed attention.
      </p>
    </div>

    <div class="takeaways-block mt-5">
      <div class="takeaways-content">
        <h4 class="takeaways-title" style="color: white !important;">Takeaways.</h4>
        <p class="takeaways-text" style="font-size: 1.2rem;">
          1. ELMUR preserves task-relevant information across horizons far beyond the attention window;<br>
          2. ELMUR generalizes seamlessly to unseen sequence lengths;<br>
          3. ELMUR improves robotic manipulation with pixel observations;<br>
          4. ELMUR scales to diverse POPGym tasks without trading off reactive control.
        </p>
      </div>
    </div>
  </div>
</section>




<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <!-- TODO: Replace with your poster PDF -->
      <iframe  src="static/images/elmur-corl-poster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@misc{cherepanov2025elmurexternallayermemory,
      title={ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL},
      author={Egor Cherepanov and Alexey K. Kovalev and Aleksandr I. Panov},
      year={2025},
      eprint={2510.07151},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2510.07151},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  <!-- Like System JavaScript -->
  <script>
    // Like system functionality
    let totalLikes = parseInt(localStorage.getItem('elmur_total_likes')) || 0;
    let likedUsers = JSON.parse(localStorage.getItem('elmur_liked_users')) || {};
    let currentUserId = generateUserId();

    // Generate unique user ID based on browser fingerprint
    function generateUserId() {
      let fingerprint = '';
      if (navigator.userAgent) fingerprint += navigator.userAgent;
      if (navigator.language) fingerprint += navigator.language;
      if (screen.width && screen.height) fingerprint += screen.width + 'x' + screen.height;

      // Simple hash function
      let hash = 0;
      for (let i = 0; i < fingerprint.length; i++) {
        const char = fingerprint.charCodeAt(i);
        hash = ((hash << 5) - hash) + char;
        hash = hash & hash; // Convert to 32-bit integer
      }

      return Math.abs(hash).toString(36);
    }

    // Check if current user has already liked
    function hasUserLiked() {
      return likedUsers[currentUserId] === true;
    }

    // Toggle like functionality
    function toggleLike() {
      const likeBtn = document.getElementById('likeBtn');
      const likeIcon = document.getElementById('likeIcon');
      const likeText = document.getElementById('likeText');

      if (hasUserLiked()) {
        // User already liked - don't allow another like
        likeIcon.className = 'far fa-heart';
        likeIcon.style.color = '#e53e3e';
        likeText.textContent = 'You already liked this work!';
        likeText.style.color = '#e53e3e';
        return;
      }

      // Add like
      totalLikes++;
      likedUsers[currentUserId] = true;

      // Update display
      likeIcon.className = 'fas fa-heart';
      likeIcon.style.color = '#e53e3e';
      likeText.textContent = 'Thank you for your like! ❤️';
      likeText.style.color = '#28a745';

      // Save to localStorage (stats only visible to admin)
      localStorage.setItem('elmur_total_likes', totalLikes.toString());
      localStorage.setItem('elmur_liked_users', JSON.stringify(likedUsers));

      // Animation effect
      likeBtn.style.transform = 'scale(1.05)';
      setTimeout(() => {
        likeBtn.style.transform = 'scale(1)';
      }, 200);
    }

    // Initialize like button state on page load
    document.addEventListener('DOMContentLoaded', function() {
      const likeIcon = document.getElementById('likeIcon');
      const likeText = document.getElementById('likeText');

      if (hasUserLiked()) {
        likeIcon.className = 'fas fa-heart';
        likeText.textContent = 'You already liked this work!';
        likeText.style.color = '#e53e3e';
      }

      // Show admin stats if special key combination is pressed (Ctrl+Shift+L)
      document.addEventListener('keydown', function(e) {
        if (e.ctrlKey && e.shiftKey && e.key === 'L') {
          showAdminStats();
        }
      });

      // Initialize scroll animations and interactive elements
      initScrollAnimations();
      initInteractiveElements();
    });

    // Add smooth scroll animations for sections
    function initScrollAnimations() {
      const observerOptions = {
        threshold: 0.1,
        rootMargin: '0px 0px -50px 0px'
      };

      const observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            entry.target.style.opacity = '1';
            entry.target.style.transform = 'translateY(0)';
          }
        });
      }, observerOptions);

      // Observe all sections and make them initially invisible
      document.querySelectorAll('.section, .hero').forEach(section => {
        section.style.opacity = '0';
        section.style.transform = 'translateY(30px)';
        section.style.transition = 'opacity 0.6s ease, transform 0.6s ease';
        observer.observe(section);
      });
    }

    // Initialize interactive elements
    function initInteractiveElements() {
      // Add hover effects to interactive images
      const interactiveImages = document.querySelectorAll('.interactive-image');
      interactiveImages.forEach(img => {
        img.addEventListener('mouseenter', function() {
          const overlay = this.parentElement.querySelector('.image-overlay');
          if (overlay) {
            overlay.style.opacity = '1';
          }
        });

        img.addEventListener('mouseleave', function() {
          const overlay = this.parentElement.querySelector('.image-overlay');
          if (overlay) {
            overlay.style.opacity = '0';
          }
        });
      });
    }

    // Admin function to view statistics (only accessible via Ctrl+Shift+L)
    function showAdminStats() {
      const userCount = Object.keys(likedUsers).length;
      const message = `ELMUR Like Statistics:\n\nTotal Likes: ${totalLikes}\nUnique Users: ${userCount}\n\nThis information is only visible to the site administrator.`;

      // Show in console for admin
      console.log('%c' + message, 'color: #2563eb; font-size: 16px; font-weight: bold;');

      // Also show alert for convenience
      alert(`ELMUR Like Statistics:\n\nTotal Likes: ${totalLikes}\nUnique Users: ${userCount}`);

      // Log individual user data for admin analysis
      console.log('Liked Users Data:', likedUsers);
    }

    // Make showAdminStats available globally for admin access
    window.showAdminStats = showAdminStats;
  </script>

  </body>
  </html>
